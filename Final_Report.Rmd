---
title: Final Report for the work in Miami Crime Dataset. Hypothesis made , followed
  by performing exploratory data analysis and finally drawing conclusions and supporting
  with suitable diagrams.
author: "Soumya Ganguly"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cran.rstudio.com/"))
```

### Introduction


### Data

The dataset used is titled as "Regression Practice Dataset, Miami Public Safety". As the title suggests, it is a dataset related to public safety in the city of Miami, Florida, and is designed for use in regression analysis.

Here is some information about the dataset:

[1] The dataset includes 14 columns, with over 22,000 rows of data. 
[2] Each row corresponds to a specific incident of crime or public safety concern in Miami.
[3] Each column provides different information about the incident, such as the date, location, and       type of offense.
[4] The dataset is designed for use in regression analysis, which means that it is intended to be        used to identify patterns and relationships in the data, with the goal of predicting future          trends or outcomes. 
[5] The dataset is publicly available on GitHub and was created by an individual named                   routineactivity, who has shared it for educational and research purposes. It is not clear whether     this dataset has been used in any published research or academic work. Overall, this dataset         provides a useful resource for those interested in analyzing patterns and trends in public safety     incidents in the city of Miami. By exploring the data and performing regression analysis, the        researchers and analysts may be able to identify patterns and relationships that could inform        policy decisions or help improve public safety in the city.

### Methods: Research Questions, Hypothesis and approaches

## Formulation of Hypothesis -\> Steps to follow

Step 1: Hypothesis Generation(Before studying the dataset)

Attempt to consider variables which can be possibly present in the dataset. The result will be a set of variables which is present in the dataset and also some parameters which is not present in the concerned
dataset.

Output: To make a decision or prediction about a possible future crime or related assumptions as per our research question the following variables might be helpful;

1.  Place, Date, Time and Area of occurence: It will help us understand the trend or pattern of occurence of the crime.

2.  Data like area-type for crime might be useful. Type of area like urban , rural, city center, close to airport, tourist spots, party area or downtown for example.

3.  Type of residents/people in that area: if there is data about financial status, age group, employment status of the residents or people frequently observed in the area; it might be useful.

4.  Location of the area; proximity to probable escape routes(border areas and port regions)

5.  Types of Crime: like Fraud, Assault, Murder etc

6.  Frequency of each types of crime in that area

7.  Police to citizen ratio in the area of crime

8.  Availability of CCTV or video evidence

9.  Population density of criminals: If there is a possibility to check the number or people with past criminal records and compare with total population

Step 2: Load and Compare-\> Load the dataset and compare with the hypothesis(educated guess) you have made


Step 3: Data Cleaning-\> Identify the columns/variables which are matching with hypothesis and contains significant values to proceeed further. As for the remaining columns ignore/drop them.

Step 4: Frame a research question

Based on analysis two research questions were formulated which are discussed further as hypothesis 1 and hypothesis 2.

### Results

## Hypothesis 1: 

## Statement: For a specific region does previous history of crime repeat itself ## over time?

Purpose and approach: We can consider crime counts, crime logs and also compare data in two different time frames and try to find out common trends.

#Null and Alternative Hypotheses

h0 -\> Previous crime history of certain crime type do not influence subsequent crimes in an area

ha -\> Previous crime history of certain crime type   influences subsequent crimes in an area

## Installing package dependencies

```{r "Installation and directory setup", include=FALSE}

# Install Packages
install.packages("contrib.url")
install.packages("caret")
install.packages("car")
install.packages("dplyr")
install.packages("forecast")
install.packages("ggplot2")
install.packages("lubridate")
install.packages("nortest")
install.packages("rgdal")
install.packages("sf")
install.packages("spdep")
install.packages("spatstat")
install.packages("stars")
install.packages("tidyverse")

# Load the libraries
library(caret)
library(car)
library(dplyr)
library(forecast)
library(ggplot2)
library(lubridate)
library(nortest)
library(rgdal)
library(sf)
library(spdep)
library(spatstat)
library(stars)
library(tidyverse)

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")

```


```{r "Hypothesis 1 => Approach 1"}

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")


# Load Geo package and read the required layers and working on the hypothesis
miami_data_violent_crimes <- st_read("com_police_data.gpkg", layer = "com_violent_crime_2021_22") %>% 
  st_transform(4326) %>% 
  st_make_valid() # Make geometries valid


# Filter data to focus on a specific region
region <- st_bbox(c(xmin=-80.31578, ymin=25.70949, xmax=-80.15572, ymax=25.85503), crs=4326) # Change this to the specific region you want to analyze


# Create a spatial object from the bounding box
region_sf <- st_as_sfc(region)
region_buffered <- region_sf %>% 
  st_buffer(0.01) # Adjust the buffer distance as needed


# Filter data and perform intersection
miami_filtered_violent_crimes <- miami_data_violent_crimes %>% 
  st_intersection(region_buffered)


# Plot crime locations to visualize the data
ggplot(miami_filtered_violent_crimes) +
  geom_sf()


# Calculate crime counts for different time periods and counties
time_periods <- c("2020-01-01/2021-12-31", "2022-01-01/2022-12-31") # Change these to the specific time periods you want to analyze
time_period_breaks <- as.POSIXct(unlist(strsplit(time_periods, "/")), format="%Y-%m-%d") # Convert each element of time_periods to a POSIXct object
crime_counts <- miami_filtered_violent_crimes %>% 
  group_by(year=as.numeric(substr(date_eu, 1, 4)), county=as.factor(county), time_period=cut(as.POSIXct(date_eu), breaks=time_period_breaks)) %>% 
  summarize(count=n())


# Plot crime counts over time to visualize trends
ggplot(crime_counts, aes(x=time_period, y=count, color=as.factor(year))) +
  geom_point() +
  geom_line() +
  labs(x="Time Period", y="Crime Count", color="Year")


# Create a contingency table of the counts of the two variables
# To find correlation between yearly crime counts with respect to area
cont_table_year_county <- table(crime_counts$year, crime_counts$county)
chisq_result_year_county <- chisq.test(cont_table_year_county) # Perform the chi-square test on the contingency table
chisq_result_year_county
chisq_result_year_county$p.value # Check the p-value to determine whether to reject or fail to reject the null hypothesis

# To find correlation between crime counts in an area with respect to time period
cont_table_year_period <- table(crime_counts$time_period, crime_counts$county)
chisq_result_year_period <- chisq.test(cont_table_year_period) # Perform the chi-square test on the contingency table
chisq_result_year_period
chisq_result_year_period$p.value # Check the p-value to determine whether to reject or fail to reject the null hypothesis


```


```{r "Hypothesis 1 => Approach 2 (Performing Approach 1 on specific crime type)"}

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")

# Load Geo package
miami_data <- st_read("com_police_data.gpkg") %>% 
  st_transform(4326) %>% 
  st_make_valid() # Make geometries valid

# List all layer names and their type in the data source
st_layers("com_police_data.gpkg")

# Load Geo package and read the required layers and working on the hypothesis
miami_data_violent_crimes <- st_read("com_police_data.gpkg", layer = "com_violent_crime_2021_22") %>% 
  st_transform(4326) %>% 
  st_make_valid() # Make geometries valid

colnames(miami_data_violent_crimes)

# Get bounding box for entire dataset
bbox_violent_crimes <- st_bbox(miami_data_violent_crimes)

# Print bounding box
bbox_violent_crimes

crimes <- unique(miami_data_violent_crimes$crime_type)
print(crimes)

# Filter data to focus on a specific region and crime type 
crime_name <- "SIMPLE ASSAULT" # Change this to the specific crime type you want to analyze
region <- st_bbox(c(xmin=-80.31578, ymin=25.70949, xmax=-80.15572, ymax=25.85503), crs=4326) # Change this to the specific region you want to analyze

# Create a spatial object from the bounding box
region_sf <- st_as_sfc(region)
region_buffered <- region_sf %>% 
  st_buffer(0.01) # Adjust the buffer distance as needed

# Filter data and perform intersection
miami_filtered_violent_crimes <- miami_data_violent_crimes %>% 
  filter(crime_type==crime_name) %>% 
  st_intersection(region_buffered)


# Plot crime locations to visualize the data
ggplot(miami_filtered_violent_crimes) +
  geom_sf()

# Calculate crime counts for different time periods and counties
time_periods <- c("2021-11-28/2021-12-28", "2022-01-23/2022-03-31") # Change these to the specific time periods you want to analyze
time_period_breaks <- as.POSIXct(unlist(strsplit(time_periods, "/")), format="%Y-%m-%d") # Convert each element of time_periods to a POSIXct object
crime_counts <- miami_filtered_violent_crimes %>% 
  group_by(year=as.numeric(substr(date_eu, 1, 4)), county=as.factor(county), time_period=cut(as.POSIXct(date_eu), breaks=time_period_breaks)) %>% 
  summarize(count=n())

# Plot crime counts over time to visualize trends
ggplot(crime_counts, aes(x=time_period, y=count, color=as.factor(year))) +
  geom_point() +
  geom_line() +
  labs(x="Time Period", y="Crime Count", color="Year")



# Create a contingency table of the counts of the two variables

# To find correlation between yearly crime counts with respect to area
cont_table_year_county <- table(crime_counts$year, crime_counts$county)
chisq_result_year_county <- chisq.test(cont_table_year_county) 

# Perform the chi-square test on the contingency table
chisq_result_year_county

# Check the p-value to determine whether to reject or fail to reject the null hypothesis
chisq_result_year_county$p.value 





# To find correlation between crime counts in an area with respect to time period
cont_table_year_period <- table(crime_counts$time_period, crime_counts$county)
chisq_result_year_period <- chisq.test(cont_table_year_period)  

# Perform the chi-square test on the contingency table
chisq_result_year_period

# Check the p-value to determine whether to reject or fail to reject the null hypothesis
chisq_result_year_period$p.value 



```


```{r "Hypothesis 1 => Approach 3" }

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")


# Load Geo package and read the required layers and working on the hypothesis
miami_data <- st_read("com_police_data.gpkg", layer = "com_violent_crime_2021_22") %>% 
  st_transform(4326) %>% 
  st_make_valid() # Make geometries valid

# Filter the dataset to only include the crime type and area of interest
crime_name <- "SIMPLE ASSAULT"
area <- "City of Miami"
miami_data_filtered <- miami_data %>%
  filter(crime_type == crime_name & county == area)

colnames(miami_data_filtered)

# Calculating incident count every week
miami_data_filtered <- miami_data_filtered %>%
  group_by(week) %>%
  mutate(incident_count = n()) %>%
  ungroup()
# Converting the data frame into a time-series object
miami_df_ts <- ts(miami_data_filtered$incident_count, frequency = 7) # assuming weekly data

# Create a time series plot of the crime counts
plot(miami_df_ts, main = paste0("Crime Counts for ", crime_name, " in ", area))

# Fit an ARIMA model to the time series data
arima_model <- auto.arima(miami_df_ts)

# Check the model summary to see the coefficients and significance levels
summary(arima_model)

# Use the model to make predictions for the next 4 weeks
predictions <- forecast(arima_model, h = 4)

# Use hypothesis testing to compare the mean crime count during periods with a high previous # crime history to the mean crime count during periods with a low previous crime history
# For example, we could split the time series data into two groups based on whether the     # crime count in the previous time period was above or below the median, and compare the    # mean crime count for the two groups

prev_crime_history <- miami_df_ts[-1]
prev_crime_median <- median(prev_crime_history)
low_prev_crime <- prev_crime_history[prev_crime_history <= prev_crime_median]
high_prev_crime <- prev_crime_history[prev_crime_history > prev_crime_median]
mean(low_prev_crime)
mean(high_prev_crime)
t.test(low_prev_crime, high_prev_crime)

```



```{r "Hypothesis 1 => Approach 4" }

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")


# Load Geo package and read the required layers and working on the hypothesis
miami_data <- st_read("com_police_data.gpkg", layer = "com_violent_crime_2021_22") %>% 
  st_transform(4326) %>% 
  st_make_valid() # Make geometries valid

# Check the structure and summary statistics of the dataset
str(miami_data)
summary(miami_data)

miami_data <- miami_data %>%
  group_by(date_eu) %>%
  mutate(incident_count = n()) %>%
  ungroup()

# Create a linear regression model to predict incident_count based on other variables
model <- lm(incident_count ~ weekday + county + hour + latitude + longitude, data = miami_data)

# Check the model summary to see the coefficients and significance levels
summary(model)

# Check the normality of residuals using a Q-Q plot and a Anderson Darling test
qqPlot(model, main="Q-Q Plot") # requires the car library
ad.test(residuals(model))

# Check the linearity of the model using a partial residual plot
crPlots(model) # requires the car library

```


## Hypothesis 2:

## Statement: Proximity to an area of interest affects(increase) the overall crime ## rate for the area?

## Purpose and approach to second hypothesis:
The purpose of this analysis is to determine if proximity to an area of interest has any effect on the overall crime rate in an area. In this analysis, we will investigate whether the presence of a bank or vault or museum within a certain distance (e.g. 500m or 1km) can be considered as a motivation for a burglary or robbery, and whether this proximity affects the overall crime rate in the area.

#Null and Alternative Hypotheses

H0: Proximity to an area of interest does not affect the crime rate.

Ha: Proximity to an area of interest affects the crime rate.

## In this section , data from 3 different layers were joined and suitable columns
## were extracted. After that frequency/incident_count column was added to proceed ## further with the regression model.

```{r "Hypothesis 2" }

# Setting current working directory
setwd("D:/MyWorkspace/AOSD-Course/R-Workspace/MiamiCrimeDataAnalysis/data")


# Loading crime data and transformation
miami_data <- st_read("com_police_data.gpkg", layer = "com_violent_crime_2021_22") %>% 
  st_transform(4326) %>% 
  st_make_valid()

# Load the neighbourhood dataset
layer_nhoods <- st_read("com_police_data.gpkg", layer = "com_nhoods") %>% 
  st_transform(4326) %>% 
  st_make_valid()

# Join the two datasets based on spatial intersection
miami_data_with_neighbourhood <- st_join(layer_nhoods, miami_data)

# Renaming LABEL column in miami_data_with_neighbourhood to neighbourhood
miami_data_with_neighbourhood <- miami_data_with_neighbourhood %>% 
  rename(neighbourhood = LABEL)

# Load the aggregated layer dataset
layer_aggregated_data <- read_sf("com_police_data.gpkg", layer = "com_aggregated_data", quiet = TRUE)

# Join the two datasets based on the common attribute neighbourhood
miami_crime_data_with_nhoods_prox <- left_join(miami_data_with_neighbourhood, layer_aggregated_data, by = "neighbourhood")

# Filter dataset for a specific crime (e.g., "Robbery")
specific_crime <- "ROBBERY / ARMED W OTHER THAN DEADLY WEAPON"
filtered_data <- miami_crime_data_with_nhoods_prox %>%
  filter(crime_type == specific_crime)

# Create a new column "incident_count" based on number of crimes per neighborhood
miami_crime_data_optimized <- filtered_data %>%
  group_by(neighbourhood, crime_type, county, geom, dist_bank, dist_convenience, dist_bars, dist450m_bank) %>%
  summarise(incident_count = n())

# Remove any missing data
miami_crime_data_optimized <- na.omit(miami_crime_data_optimized)


# Evaluation and Hypothesis Testing
# Fit the regression model
model <- lm(incident_count ~ dist_bars + dist_convenience, data = miami_crime_data_optimized)

# Print the model summary
summary(model)

# Test the hypothesis using ANOVA
anova(model)

# Perform the Wilcoxon rank-sum test
wilcox.test(incident_count ~ dist450m_bank, data = miami_crime_data_optimized)

# Visualize the results
ggplot(miami_crime_data_optimized, aes(x = dist_bank, y = incident_count)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(title = "Relationship between Crime Incident Count and distance to nearest bank")

```


#### Conclusions

### Hypothesis 1: 

Approach 1 and 2: Nothing conclusive could be drawn from the two approaches. Chisquared test suggested a possibility of incorrect evaluation. Also the p values received were much more than 0.05. So the Null Hypothesis cannot be totally ignored.

## Approach 3: 

#Conclusion 1:  

The ARIMA model results suggest that the time series data has a non-zero mean and shows a negative trend. The coefficients of the ARIMA model indicate that there is a significant negative autocorrelation at lag 1 for both the AR and seasonal AR terms.

The training set error measures suggest that the ARIMA model has a low error, which is further supported by the log likelihood, AIC, and BIC values.



# Conclusion 2: 

The Welch two-sample t-test results indicate that there is a significant difference in means between the low_prev_crime and high_prev_crime groups. The p-value is very small (< 2.2e-16), indicating that the null hypothesis of no difference in means can be rejected in favor of the alternative hypothesis of a difference in means. The confidence interval also suggests that the true difference in means between the two groups is not zero.

Overall, these results provide evidence in favor of the alternate hypothesis that there is a difference in crime rates between the low_prev_crime and high_prev_crime groups.

## Approach 4:

The following conclusions can be drawn in favor or against the alternate hypothesis:

The coefficients for some of the variables, such as weekday, county, and latitude, are statistically significant (indicated by the ***, **, or * symbols) at a 5% significance level, which suggests that these variables have a significant effect on the incident_count.

The p-value for the F-statistic is less than 2.2e-16, which indicates that the overall model is significant.

The Adjusted R-squared value is 0.08994, which means that the model explains only 8.994% of the variation in the incident_count.

The Anderson-Darling normality test for the residuals shows a p-value less than 2.2e-16, which indicates that the residuals are not normally distributed. This suggests that the linear regression assumptions may not hold, and the results should be interpreted with caution.

In summary, the model suggests that the weekday, county, and latitude variables have a significant effect on the incident_count. However, the low Adjusted R-squared value and the violation of normality assumptions suggest that the model may not fit the data well. Therefore, further investigation and analysis may be required to draw reliable conclusions.

To summarize, Approach 3 can strongly support in favor of Hypothesis 1. 


### Hypothesis 2:

## Approach 1:

# Conclusion: 

Based on the output, we can draw the following conclusions:

The coefficients of both dist_bars and dist_convenience are positive, indicating that as proximity to bars or convenience stores increases, so does the incident count.

The p-value for dist_bars is 0.087, which is greater than the significance level of 0.05. Therefore, we cannot reject the null hypothesis that proximity to bars does not affect the crime rate.

The p-value for dist_convenience is 0.124, which is also greater than the significance level of 0.05. Therefore, we cannot reject the null hypothesis that proximity to convenience stores does not affect the crime rate.

The Wilcoxon rank sum test shows a p-value of 7.842e-06, which is less than the significance level of 0.05. Therefore, we reject the null hypothesis and conclude that there is a significant difference in incident counts between areas close to and far from banks.

Overall, the results suggest that proximity to bars or convenience stores does not significantly affect the crime rate, but proximity to banks does. However, it is important to note that the adjusted R-squared value is very low, indicating that the model does not explain much of the variation in incident counts. Therefore, further analysis and modeling may be necessary to fully understand the relationship between proximity to different types of areas and crime rates in Miami.


#### References

[1] ARIMA modeling: "Time Series Analysis and Its Applications: With R Examples" by Shumway, R. H., & Stoffer, D. S. (2017).
URL: https://www.researchgate.net/publication/265365840_Time_Series_Analysis_and_Its_Applications_With_R_Examples


[2] linear regression and its assumptions: "Linear Models in Statistics".
URL: https://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf

[3] A beginner's guide to understanding p-values: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/

[4] P Values, Hypothesis Testing, and Statistical Significance: A Brief Tutorial:             https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5808128/

[5] Official Documentation for ARC GIS PRO was helpful
https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/regression-analysis-basics.htm

[6] Watched a tutorial for ggplot2: https://towardsdatascience.com/data-visualization-with-ggplot2-db04c4956236








